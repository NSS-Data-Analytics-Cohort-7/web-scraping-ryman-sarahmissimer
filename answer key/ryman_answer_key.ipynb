{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as BS \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the page we're pulling from\n",
    "URL = 'https://ryman.com/events/'\n",
    "\n",
    "# grabs all the code that makes the website\n",
    "response = requests.get(URL)\n",
    "\n",
    "# turns it into soup so we can use BS functions to find what we are looking for\n",
    "soup = BS(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Start by using either the inspector or by viewing the page source. Can you identify a tag that might be helpful for finding the names of all performers? For now, just worry about the headliner and don't worry about the opener. (Eg. For Vince Gill, featuring Wendy Moten, we only care about Vince Gill.) Make use of this to create a list containing just the names of each inductee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example element that holds the information for a name:\n",
    "# <a class=\"tribe-event-url\" href=\"https://ryman.com/event/lynyrd-skynyrd/\" title=\"Lynyrd Skynyrd\" rel=\"bookmark\">Lynyrd Skynyrd</a>\n",
    "\n",
    "# when inspecting, the names are all in 'a' tags.\n",
    "# however, a ton of stuff is stored in 'a' tags, not just the stuff we want\n",
    "# the attribute 'class' with the value 'tribe-event-url' is unique to the main act names\n",
    "# using findAll, the first argument is the tag we're searching for\n",
    "# the second is a dictionary with the attribute and value we want\n",
    "name_soup = soup.findAll('a', {'class': 'tribe-event-url'})\n",
    "name_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it makes sense to run a test on the first instance of our list of elements to decide how to pull out text \n",
    "# before creating a loop / list comprehension to grab the text in every element\n",
    "name_test = name_soup[0].text\n",
    "name_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the string method .strip() to take off all the extra garbage we don't want\n",
    "# remember '\\' indicates something special is happening in code, \n",
    "# so that's how .strip() knows we can get rid of it\n",
    "name_test_clean = name_soup[0].text.strip()\n",
    "name_test_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list comprehension to run all of this code on each instance in the list\n",
    "names = [name.text.strip() for name in name_soup]\n",
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Next, try and find a tag that could be used to find the date and time for each show. Extract these into two lists, one containing the date and the other containing the time. (Eg. THURSDAY, AUGUST 4, 2022 AT 8:00 PM CDT should be split into August 4, 2022 and 8:00 PM CDT.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example element that holds the information for a name:\n",
    "# <time datetime=\"2022-11-13 07:30:00 CST\">Sunday, November 13, 2022 at 7:30 PM CST</time>\n",
    "# the tag here is 'time'\n",
    "# luckily in this case, the only tag that holds time is what we're looking for, so we don't need to worry about attributes\n",
    "time_soup = soup.findAll('time')\n",
    "time_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on one element\n",
    "time_soup[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list comprehension to get all of them\n",
    "times = [time.text for time in time_soup]\n",
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing code on the first item in our list 'times' before creating a loop to make sure it will work\n",
    "# the first index [0] is to grab the first item in times, the second indeces [0] and [1] are to indicate\n",
    "# the first half of the split will be our date and the second half will be our time\n",
    "# (look at the output of the last print statement to see what the split looks like before we use the indeces to break it\n",
    "# into two variables)\n",
    "test_split_date = times[0].split(\" at \")[0]\n",
    "test_split_oclock = times[0].split(\" at \")[1]\n",
    "print(test_split_date)\n",
    "print(test_split_oclock)\n",
    "\n",
    "# to save time, you can also use this method:\n",
    "test_split_date_2, test_split_oclock_2 = times[0].split(\" at \")\n",
    "print(test_split_date_2)\n",
    "print(test_split_oclock_2)\n",
    "\n",
    "print(times[0].split(\" at \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop to split the dates and times into two separate lists\n",
    "\n",
    "# initiate empty lists to append to\n",
    "dates = []\n",
    "oclocks = []\n",
    "\n",
    "# iterate over the times list we created in the cell above\n",
    "for time in times:\n",
    "\n",
    "# outside of a loop, this is where we would say 'date = time.split(\" at \")[0]' to create a variable with that one value\n",
    "# when running a loop, a common method of collecting these individual variables into a list is by changing this code to \n",
    "# the following using .append() in conjunction with the empty list we created outside the loop\n",
    "    dates.append(time.split(\" at \")[0])\n",
    "    oclocks.append(time.split(\" at \")[1])\n",
    "    \n",
    "print(dates)\n",
    "print(oclocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same code as above without all the comments so it is easier to read\n",
    "\n",
    "dates = []\n",
    "oclocks = []\n",
    "\n",
    "for time in times:\n",
    "\n",
    "    dates.append(time.split(\" at \")[0])\n",
    "    oclocks.append(time.split(\" at \")[1])\n",
    "    \n",
    "print(dates)\n",
    "print(oclocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take the two lists you created on parts 1 and 2 and convert it into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a dictionary with pd.DataFrame to turn the lists into columns and give the columns names\n",
    "events = pd.DataFrame(data = {\n",
    "    'act': names,\n",
    "    'date': dates,\n",
    "    'times': oclocks\n",
    "})\n",
    "\n",
    "events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Now, you need to take what you created for the first page and apply it across multiple rest of the pages so that you can scrape all inductees. Notice how the url changes when you click the \"More Events\" button at the top of the page. Check that the code that you wrote for the first page still works for page 2. Once you have verified that your code will still work, write a for loop that will cycle through the first five pages of events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate empty lists to append to\n",
    "names = []\n",
    "dates = []\n",
    "oclocks = []\n",
    "\n",
    "# the range of pages we want to loop over\n",
    "for page in range(1,5):\n",
    "    \n",
    "# since we know the base URL is the same over all pages with the only difference being the page going up by one each page,\n",
    "# we can use an f string to write out the URL with the variable 'page' that will go up by one in our range on each iteration\n",
    "    url = f'https://ryman.com/events/list/?tribe_event_display=list&tribe_paged={page}'\n",
    "\n",
    "# this is always the same two steps to grab the website code and turn it into soup\n",
    "# putting it in the loop means we will get the soup for the page the loop is on and then the soup will be replaced\n",
    "# on the next loop through\n",
    "    response = requests.get(URL)\n",
    "    soup = BS(response.text)\n",
    "    \n",
    "# this is the exact code we used to get our soup of just names in question one\n",
    "    name_soup = soup.findAll('a', {'class': 'tribe-event-url'})\n",
    "# the reason I am using extend this time instead of append is a little much to explain in a comment,\n",
    "# but I am happy to explain any time\n",
    "# we are able to use the same exact list comprehension we used in question one inside the .extend()\n",
    "    names.extend([name.text.strip() for name in name_soup])\n",
    "\n",
    "# again, same code as question two\n",
    "    time_soup = soup.findAll('time')\n",
    "    times = [time.text for time in time_soup]\n",
    "\n",
    "# same exact loop we used in question two\n",
    "    for time in times:\n",
    "        dates.append(time.split(\" at \")[0])\n",
    "        oclocks.append(time.split(\" at \")[1])\n",
    "\n",
    "# creation of our final dataframe using the lists we just got finished creating in the nested loop above\n",
    "# it is important to make sure this step starts all the way to the left of the cell block instead of being indented\n",
    "# this is how python knows this is not to be run in the loop, but instead after the loop is finished running\n",
    "five_pages_df = pd.DataFrame({\n",
    "    'act': names,\n",
    "    'dates': dates,\n",
    "    'oclocks': oclocks\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same code as above without all the comments so it is easier to read\n",
    "\n",
    "names = []\n",
    "dates = []\n",
    "oclocks = []\n",
    "\n",
    "for page in range(1,5):\n",
    "\n",
    "    url = f'https://ryman.com/events/list/?tribe_event_display=list&tribe_paged={page}'\n",
    "    response = requests.get(URL)\n",
    "    soup = BS(response.text)\n",
    "    \n",
    "    name_soup = soup.findAll('a', {'class': 'tribe-event-url'})\n",
    "    names.extend([name.text.strip() for name in name_soup])\n",
    "\n",
    "    time_soup = soup.findAll('time')\n",
    "    times = [time.text for time in time_soup]\n",
    "\n",
    "    for time in times:\n",
    "        dates.append(time.split(\" at \")[0])\n",
    "        oclocks.append(time.split(\" at \")[1])\n",
    "\n",
    "five_pages_df = pd.DataFrame({\n",
    "    'act': names,\n",
    "    'dates': dates,\n",
    "    'oclocks': oclocks\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_pages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
